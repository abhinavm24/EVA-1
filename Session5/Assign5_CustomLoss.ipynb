{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assign5_CustomLoss2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9aAeoiGWWrY",
        "colab_type": "text"
      },
      "source": [
        "####  **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJyVpgSxHt4",
        "colab_type": "code",
        "outputId": "21b6906e-7814-4e24-bf46-952789dfb725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "dir=\"/content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign5/\"\n",
        "\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bugf5EB4WaA2",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets\n",
        "Plotting a sample image from the dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Y9Va-xxMXG",
        "colab_type": "code",
        "outputId": "4eee2d24-a620-459e-c42f-cf4c6370fabd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print (x_train.shape)\n",
        "\n",
        "plt.imshow(x_train[1], cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n",
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc21c7e2f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADjBJREFUeJzt3X+MVfWZx/HPoy1EpRi1WRxFl26D\nTRqjg4zEP8jKumvjIgk0RoUYh6bNDn+UxJqNqdpRSdaNjVE2aiKRKimsLFBFAzbr0i5jtE1M44is\nP7eVbagdHBkRI0NMZIVn/7iHzaBzv+dy77n3nJnn/Uomc+957rnn8Tofzj33e+75mrsLQDynlN0A\ngHIQfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQX2lkxszM04nBNrM3a2Rx7W05zeza8zs92a2\nx8xub+W5AHSWNXtuv5mdKukPkq6WNCTpFUnL3P3txDrs+YE268Sef56kPe7+R3c/ImmzpMUtPB+A\nDmol/OdL+vOY+0PZshOYWZ+ZDZrZYAvbAlCwtn/g5+5rJa2VeNsPVEkre/59ki4Yc39mtgzABNBK\n+F+RNNvMvmFmUyQtlbS9mLYAtFvTb/vd/XMzWylph6RTJa1z97cK6wxAWzU91NfUxjjmB9quIyf5\nAJi4CD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8E\nRfiBoAg/EBThB4Ii/EBQhB8IqqNTdGPymTt3brK+cuXKurXe3t7kuhs2bEjWH3nkkWR9165dyXp0\n7PmBoAg/EBThB4Ii/EBQhB8IivADQRF+IKiWZuk1s72SRiUdlfS5u/fkPJ5ZeieY7u7uZH1gYCBZ\nnz59epHtnOCTTz5J1s8555y2bbvKGp2lt4iTfP7G3Q8U8DwAOoi3/UBQrYbfJf3KzF41s74iGgLQ\nGa2+7Z/v7vvM7C8k/drM/tvdXxr7gOwfBf5hACqmpT2/u+/Lfo9IelbSvHEes9bde/I+DATQWU2H\n38zOMLOvHb8t6TuS3iyqMQDt1crb/hmSnjWz48/zb+7+H4V0BaDtWhrnP+mNMc5fOfPmfelI7QRb\nt25N1s8777xkPfX3NTo6mlz3yJEjyXreOP78+fPr1vK+65+37SprdJyfoT4gKMIPBEX4gaAIPxAU\n4QeCIvxAUAz1TQKnn3563dpll12WXPfJJ59M1mfOnJmsZ+d51JX6+8obbrv//vuT9c2bNyfrqd76\n+/uT6953333JepUx1AcgifADQRF+ICjCDwRF+IGgCD8QFOEHgmKK7kngscceq1tbtmxZBzs5OXnn\nIEybNi1Zf/HFF5P1BQsW1K1dcsklyXUjYM8PBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzj8BzJ07\nN1m/9tpr69byvm+fJ28s/bnnnkvWH3jggbq1999/P7nua6+9lqx//PHHyfpVV11Vt9bq6zIZsOcH\ngiL8QFCEHwiK8ANBEX4gKMIPBEX4gaByr9tvZuskLZI04u4XZ8vOlrRF0ixJeyXd4O7pQVdx3f56\nuru7k/WBgYFkffr06U1v+/nnn0/W864HcOWVVybrqe/NP/7448l1P/zww2Q9z9GjR+vWPv300+S6\nef9deXMOlKnI6/b/XNI1X1h2u6Sd7j5b0s7sPoAJJDf87v6SpINfWLxY0vrs9npJSwruC0CbNXvM\nP8Pdh7PbH0iaUVA/ADqk5XP73d1Tx/Jm1iepr9XtAChWs3v+/WbWJUnZ75F6D3T3te7e4+49TW4L\nQBs0G/7tkpZnt5dL2lZMOwA6JTf8ZrZJ0suSvmVmQ2b2A0k/lXS1mb0r6e+y+wAmkNxx/kI3FnSc\n/6KLLkrW77nnnmR96dKlyfqBAwfq1oaHh+vWJOnee+9N1p9++ulkvcpS4/x5f/dbtmxJ1m+66aam\neuqEIsf5AUxChB8IivADQRF+ICjCDwRF+IGguHR3AaZOnZqspy5fLUkLFy5M1kdHR5P13t7eurXB\nwcHkuqeddlqyHtWFF15Ydgttx54fCIrwA0ERfiAowg8ERfiBoAg/EBThB4JinL8Ac+bMSdbzxvHz\nLF68OFnPm0YbGA97fiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IinH+AqxevTpZN0tfSTlvnJ5x/Oac\nckr9fduxY8c62Ek1secHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaByx/nNbJ2kRZJG3P3ibNkqSf8g\n6cPsYXe6+7+3q8kqWLRoUd1ad3d3ct286aC3b9/eVE9IS43l5/0/2b17d9HtVE4je/6fS7pmnOX/\n4u7d2c+kDj4wGeWG391fknSwA70A6KBWjvlXmtnrZrbOzM4qrCMAHdFs+NdI+qakbknDkh6s90Az\n6zOzQTNLTxoHoKOaCr+773f3o+5+TNLPJM1LPHatu/e4e0+zTQIoXlPhN7OuMXe/K+nNYtoB0CmN\nDPVtkrRA0tfNbEjSPZIWmFm3JJe0V9KKNvYIoA1yw+/uy8ZZ/EQbeqm01Dz2U6ZMSa47MjKSrG/Z\nsqWpnia7qVOnJuurVq1q+rkHBgaS9TvuuKPp554oOMMPCIrwA0ERfiAowg8ERfiBoAg/EBSX7u6A\nzz77LFkfHh7uUCfVkjeU19/fn6zfdtttyfrQ0FDd2oMP1j0jXZJ0+PDhZH0yYM8PBEX4gaAIPxAU\n4QeCIvxAUIQfCIrwA0Exzt8BkS/Nnbqsed44/Y033pisb9u2LVm/7rrrkvXo2PMDQRF+ICjCDwRF\n+IGgCD8QFOEHgiL8QFCM8zfIzJqqSdKSJUuS9VtuuaWpnqrg1ltvTdbvuuuuurUzzzwzue7GjRuT\n9d7e3mQdaez5gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCo3HF+M7tA0gZJMyS5pLXu/pCZnS1pi6RZ\nkvZKusHdP25fq+Vy96ZqknTuuecm6w8//HCyvm7dumT9o48+qlu74oorkuvefPPNyfqll16arM+c\nOTNZf++99+rWduzYkVz30UcfTdbRmkb2/J9L+kd3/7akKyT90My+Lel2STvdfbakndl9ABNEbvjd\nfdjdd2W3RyW9I+l8SYslrc8etl5S+jQ2AJVyUsf8ZjZL0hxJv5M0w92PzzP1gWqHBQAmiIbP7Tez\naZK2SvqRux8aez67u7uZjXvga2Z9kvpabRRAsRra85vZV1UL/kZ3fyZbvN/MurJ6l6SR8dZ197Xu\n3uPuPUU0DKAYueG32i7+CUnvuPvqMaXtkpZnt5dLSl9KFUClWN4wlZnNl/QbSW9IOpYtvlO14/5f\nSLpQ0p9UG+o7mPNc6Y1V2PXXX1+3tmnTprZue//+/cn6oUOH6tZmz55ddDsnePnll5P1F154oW7t\n7rvvLrodSHL39HfMM7nH/O7+W0n1nuxvT6YpANXBGX5AUIQfCIrwA0ERfiAowg8ERfiBoHLH+Qvd\n2AQe5099dfWpp55Krnv55Ze3tO28S4O38v8w9XVgSdq8eXOyPpEvOz5ZNTrOz54fCIrwA0ERfiAo\nwg8ERfiBoAg/EBThB4JinL8AXV1dyfqKFSuS9f7+/mS9lXH+hx56KLnumjVrkvU9e/Yk66gexvkB\nJBF+ICjCDwRF+IGgCD8QFOEHgiL8QFCM8wOTDOP8AJIIPxAU4QeCIvxAUIQfCIrwA0ERfiCo3PCb\n2QVm9oKZvW1mb5nZLdnyVWa2z8x2Zz8L298ugKLknuRjZl2Sutx9l5l9TdKrkpZIukHSYXd/oOGN\ncZIP0HaNnuTzlQaeaFjScHZ71MzekXR+a+0BKNtJHfOb2SxJcyT9Llu00sxeN7N1ZnZWnXX6zGzQ\nzAZb6hRAoRo+t9/Mpkl6UdI/u/szZjZD0gFJLumfVDs0+H7Oc/C2H2izRt/2NxR+M/uqpF9K2uHu\nq8epz5L0S3e/OOd5CD/QZoV9scdql459QtI7Y4OffRB43HclvXmyTQIoTyOf9s+X9BtJb0g6li2+\nU9IySd2qve3fK2lF9uFg6rnY8wNtVujb/qIQfqD9+D4/gCTCDwRF+IGgCD8QFOEHgiL8QFCEHwiK\n8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAULkX8CzYAUl/GnP/69myKqpqb1XtS6K3ZhXZ2182+sCO\nfp//Sxs3G3T3ntIaSKhqb1XtS6K3ZpXVG2/7gaAIPxBU2eFfW/L2U6raW1X7kuitWaX0VuoxP4Dy\nlL3nB1CSUsJvZteY2e/NbI+Z3V5GD/WY2V4zeyObebjUKcayadBGzOzNMcvONrNfm9m72e9xp0kr\nqbdKzNycmFm61NeuajNed/xtv5mdKukPkq6WNCTpFUnL3P3tjjZSh5ntldTj7qWPCZvZX0s6LGnD\n8dmQzOx+SQfd/afZP5xnufuPK9LbKp3kzM1t6q3ezNLfU4mvXZEzXhehjD3/PEl73P2P7n5E0mZJ\ni0voo/Lc/SVJB7+weLGk9dnt9ar98XRcnd4qwd2H3X1XdntU0vGZpUt97RJ9laKM8J8v6c9j7g+p\nWlN+u6RfmdmrZtZXdjPjmDFmZqQPJM0os5lx5M7c3ElfmFm6Mq9dMzNeF40P/L5svrtfJunvJf0w\ne3tbSV47ZqvScM0aSd9UbRq3YUkPltlMNrP0Vkk/cvdDY2tlvnbj9FXK61ZG+PdJumDM/ZnZskpw\n933Z7xFJz6p2mFIl+49Pkpr9Him5n//n7vvd/ai7H5P0M5X42mUzS2+VtNHdn8kWl/7ajddXWa9b\nGeF/RdJsM/uGmU2RtFTS9hL6+BIzOyP7IEZmdoak76h6sw9vl7Q8u71c0rYSezlBVWZurjeztEp+\n7So347W7d/xH0kLVPvH/H0k/KaOHOn39laT/yn7eKrs3SZtUexv4v6p9NvIDSedI2inpXUn/Kens\nCvX2r6rN5vy6akHrKqm3+aq9pX9d0u7sZ2HZr12ir1JeN87wA4LiAz8gKMIPBEX4gaAIPxAU4QeC\nIvxAUIQfCIrwA0H9H/00nuWz++2XAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxDZxPhhxOgO",
        "colab_type": "code",
        "outputId": "d278d9c5-63e6-41a4-8853-7d2c8fc11ac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "X_train = x_train.reshape(x_train.shape[0], 28, 28,1)\n",
        "X_test = x_test.reshape(x_test.shape[0], 28, 28,1)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# Image standardisation\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print(X_train.shape)\n",
        "# Row 15 of second image\n",
        "print(X_train[1,15,:,0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.29803923 0.9647059  0.9882353  0.4392157  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.99215686 0.9882353  0.5803922  0.\n",
            " 0.         0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoa5K7ynWpfq",
        "colab_type": "text"
      },
      "source": [
        "One hot encoding the labels from training and test set. Each 1d label is converted to 10d sparse matrix. Eg, digit 2 becomes [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LdYiW6ixR9e",
        "colab_type": "code",
        "outputId": "7c53409b-b86d-477c-900b-bdc8fe75d431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(y_train[1])\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "Y_train[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jokeMUyIWtph",
        "colab_type": "text"
      },
      "source": [
        "#### Image normalisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC8_F-2ctU5B",
        "colab_type": "code",
        "outputId": "a1bdf5bd-6a82-4464-b3a7-d58323417172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Entire dataset mean and standard deviation\n",
        "print('Statistics train=%.3f (%.3f), test=%.3f (%.3f)' % (X_train.mean(), X_train.std(), X_test.mean(), X_test.std()))\n",
        "\n",
        "# Create generator to normalize images\n",
        "# feature-wise is per-dataset, sample-wise is per-image\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "\n",
        "# Calculate mean, var on training dataset, required if featurewise_center/featurewise_std_normalization set to True\n",
        "datagen.fit(X_train)\n",
        "print('Train data generator mean=%.3f, std=%.3f' % (datagen.mean, datagen.std))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics train=0.131 (0.308), test=0.133 (0.310)\n",
            "Train data generator mean=0.131, std=0.308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeQt9looB8EN",
        "colab_type": "code",
        "outputId": "3eccb76b-2b50-4cb9-8c6d-203d3c4f5b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "# get batch of the training set\n",
        "train_iterator = datagen.flow(X_train, Y_train, batch_size=batch_size, shuffle=True, seed=42) # default is true\n",
        "batchX, batchy = train_iterator.next()\n",
        "\n",
        "valid_iterator = datagen.flow(X_test, Y_test, batch_size=batch_size, shuffle=True, seed=42)\n",
        "\n",
        "print('Batches train=%d, test=%d' % (len(train_iterator), len(valid_iterator)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batches train=469, test=79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQtKQ1JOYklL",
        "colab_type": "code",
        "outputId": "1a7d10fb-fcba-4e3e-bb30-9edea5b9ec4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "print(batchX.shape)\n",
        "print(batchX[1,15,:,0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 28, 28, 1)\n",
            "[-0.42407304 -0.42407304 -0.42407304 -0.42407304 -0.42407304 -0.42407304\n",
            " -0.42407304 -0.42407304 -0.42407304  0.39051074  2.0069501   1.2050945\n",
            "  1.2050945   1.2050945   1.2050945  -0.42407304 -0.42407304 -0.42407304\n",
            "  2.0069501   2.821534    2.821534    1.2050945  -0.42407304 -0.42407304\n",
            " -0.42407304 -0.42407304 -0.42407304 -0.42407304]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bL3JTX3_c8-L",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(16, kernel_size=(3, 3), use_bias=False, input_shape=(28,28,1))) #26\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.1))\n",
        "\n",
        "  model.add(Conv2D(32, (3, 3), use_bias=False)) #24\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.1))\n",
        "\n",
        "  model.add(Conv2D(10, (1, 1), activation='relu', use_bias=False))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))#12\n",
        "\n",
        "  model.add(Conv2D(16, (3, 3), use_bias=False))#10\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.1))\n",
        "\n",
        "  model.add(Conv2D(16, (3, 3), use_bias=False))#8\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.1))\n",
        "\n",
        "  model.add(Conv2D(16, (3, 3), use_bias=False))#6\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.1))\n",
        "\n",
        "  model.add(Conv2D(16, (3, 3), use_bias=False))#4\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.1))\n",
        "\n",
        "  model.add(Conv2D(10, (4, 4))) \n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Activation('softmax'))\n",
        "  \n",
        "  return model\n",
        "\n",
        "# custom_model = build_model()\n",
        "# custom_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE4vJQ2od6JN",
        "colab_type": "text"
      },
      "source": [
        "#### With std. loss and LR - best val. accuracy at epoch 20 is 99.47% (train acc. 99.26%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaNYeM4HdeVM",
        "colab_type": "code",
        "outputId": "3906dcfa-473a-485c-bdd8-20c4b72ec320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        }
      },
      "source": [
        "custom_model = build_model()\n",
        "custom_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "custom_model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator),                     \n",
        "                    validation_data=valid_iterator, validation_steps=len(valid_iterator),\n",
        "                    epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 14s 30ms/step - loss: 0.3951 - acc: 0.8819 - val_loss: 0.1079 - val_acc: 0.9659\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0827 - acc: 0.9750 - val_loss: 0.0576 - val_acc: 0.9816\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0600 - acc: 0.9815 - val_loss: 0.0357 - val_acc: 0.9884\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0497 - acc: 0.9846 - val_loss: 0.0331 - val_acc: 0.9893\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0433 - acc: 0.9861 - val_loss: 0.0292 - val_acc: 0.9902\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0404 - acc: 0.9872 - val_loss: 0.0287 - val_acc: 0.9912\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0384 - acc: 0.9879 - val_loss: 0.0226 - val_acc: 0.9927\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0333 - acc: 0.9890 - val_loss: 0.0236 - val_acc: 0.9921\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0323 - acc: 0.9899 - val_loss: 0.0285 - val_acc: 0.9912\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0305 - acc: 0.9905 - val_loss: 0.0192 - val_acc: 0.9939\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0309 - acc: 0.9897 - val_loss: 0.0204 - val_acc: 0.9933\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0287 - acc: 0.9909 - val_loss: 0.0236 - val_acc: 0.9925\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0271 - acc: 0.9913 - val_loss: 0.0210 - val_acc: 0.9935\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0270 - acc: 0.9912 - val_loss: 0.0215 - val_acc: 0.9930\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0266 - acc: 0.9913 - val_loss: 0.0222 - val_acc: 0.9922\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0247 - acc: 0.9923 - val_loss: 0.0187 - val_acc: 0.9937\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0230 - acc: 0.9927 - val_loss: 0.0198 - val_acc: 0.9936\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0232 - acc: 0.9926 - val_loss: 0.0214 - val_acc: 0.9928\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0232 - acc: 0.9924 - val_loss: 0.0233 - val_acc: 0.9916\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0229 - acc: 0.9926 - val_loss: 0.0211 - val_acc: 0.9932\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc1c70e9e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqieUaLdcPyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init_lr = 0.003\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  return round(init_lr * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "new_lr = LearningRateScheduler(scheduler, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnhB3-DadQZx",
        "colab_type": "text"
      },
      "source": [
        "#### With std. loss and custom LR - best val. accuracy at epoch 19 is 99.48% (train acc. 99.45%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHJN0Xc4b3K7",
        "colab_type": "code",
        "outputId": "20d001be-7f70-4083-ec25-e3b7c11384cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "custom_model = build_model()\n",
        "custom_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=init_lr), metrics=['accuracy'])\n",
        "\n",
        "custom_model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator),                     \n",
        "                    validation_data=valid_iterator, validation_steps=len(valid_iterator),\n",
        "                    epochs=20, verbose=1, callbacks=[new_lr])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "469/469 [==============================] - 15s 32ms/step - loss: 0.2271 - acc: 0.9295 - val_loss: 0.0635 - val_acc: 0.9787\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0637 - acc: 0.9796 - val_loss: 0.0389 - val_acc: 0.9874\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0522 - acc: 0.9839 - val_loss: 0.0364 - val_acc: 0.9888\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0440 - acc: 0.9862 - val_loss: 0.0342 - val_acc: 0.9891\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0390 - acc: 0.9881 - val_loss: 0.0302 - val_acc: 0.9899\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0353 - acc: 0.9892 - val_loss: 0.0305 - val_acc: 0.9919\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0322 - acc: 0.9901 - val_loss: 0.0245 - val_acc: 0.9929\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0311 - acc: 0.9905 - val_loss: 0.0196 - val_acc: 0.9940\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0276 - acc: 0.9907 - val_loss: 0.0196 - val_acc: 0.9941\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0270 - acc: 0.9914 - val_loss: 0.0260 - val_acc: 0.9927\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0255 - acc: 0.9918 - val_loss: 0.0204 - val_acc: 0.9934\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0242 - acc: 0.9922 - val_loss: 0.0225 - val_acc: 0.9934\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0236 - acc: 0.9923 - val_loss: 0.0213 - val_acc: 0.9940\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0226 - acc: 0.9925 - val_loss: 0.0216 - val_acc: 0.9931\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0220 - acc: 0.9933 - val_loss: 0.0211 - val_acc: 0.9939\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0214 - acc: 0.9933 - val_loss: 0.0204 - val_acc: 0.9934\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0207 - acc: 0.9933 - val_loss: 0.0223 - val_acc: 0.9930\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0192 - acc: 0.9940 - val_loss: 0.0200 - val_acc: 0.9940\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0195 - acc: 0.9937 - val_loss: 0.0198 - val_acc: 0.9939\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0187 - acc: 0.9938 - val_loss: 0.0194 - val_acc: 0.9935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc1ca88bfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LumrLeKF065o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lamda=0.02\n",
        "m = X_train.shape[0]\n",
        "\n",
        "def custom_loss(lamda):\n",
        "  model_layers = custom_model.layers # type list where each el is Conv2D obj etc.\n",
        "  #print(model_layers)\n",
        "  reg_wts = 0\n",
        "  \n",
        "  for idx, layer in enumerate(model_layers):\n",
        "    layer_wts = model_layers[idx].get_weights() # type list\n",
        "    #print(len(layer_wts), layer_wts)\n",
        "    \n",
        "    if len(layer_wts) > 0: # activation, dropout layers do not have any weights\n",
        "      layer_wts = model_layers[idx].get_weights()[0] #type ndarray, 3,3,1,16 : layer1 output\n",
        "      reg_wts += np.sum(layer_wts**2)\n",
        "      \n",
        "  print(\"Reg. loss\", reg_wts)    \n",
        "  reg_wts = reg_wts * (lamda/(2*m))\n",
        "      \n",
        "  def total_loss(y_true, y_pred):   \n",
        "    return K.categorical_crossentropy(y_true, y_pred) + reg_wts\n",
        "  \n",
        "  return total_loss\n",
        "      \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwRe9rfRnz4L",
        "colab_type": "text"
      },
      "source": [
        "#### With custom loss and custom LR - 2 values of lambda were tried.\n",
        "\n",
        "1. **Lambda 0.01** - best val. accuracy at epoch 16 is 99.59% (train acc. 99.32%) This is better, so keeping this.\n",
        "2. **Lambda 0.02** - best val. accuracy at epoch 18 is 99.44% (train acc. 99.33%)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFR0IlnqtTYu",
        "colab_type": "code",
        "outputId": "cbf86a6e-b9a9-4871-8d2e-61fde7f31778",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "custom_model = build_model()\n",
        "custom_model.compile(loss=custom_loss(lamda=0.01), optimizer=Adam(lr=init_lr), metrics=['accuracy'])\n",
        "\n",
        "custom_model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator),                     \n",
        "                    validation_data=valid_iterator, validation_steps=len(valid_iterator),\n",
        "                    epochs=20, verbose=1, callbacks=[new_lr])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reg. loss 222.47710072994232\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "469/469 [==============================] - 18s 38ms/step - loss: 0.2319 - acc: 0.9282 - val_loss: 0.0625 - val_acc: 0.9817\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0611 - acc: 0.9811 - val_loss: 0.0306 - val_acc: 0.9892\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0500 - acc: 0.9845 - val_loss: 0.0314 - val_acc: 0.9895\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0410 - acc: 0.9869 - val_loss: 0.0315 - val_acc: 0.9890\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0380 - acc: 0.9878 - val_loss: 0.0350 - val_acc: 0.9894\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0343 - acc: 0.9891 - val_loss: 0.0212 - val_acc: 0.9932\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0317 - acc: 0.9901 - val_loss: 0.0217 - val_acc: 0.9926\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0299 - acc: 0.9904 - val_loss: 0.0199 - val_acc: 0.9939\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0276 - acc: 0.9912 - val_loss: 0.0162 - val_acc: 0.9949\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0268 - acc: 0.9914 - val_loss: 0.0201 - val_acc: 0.9934\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0257 - acc: 0.9919 - val_loss: 0.0173 - val_acc: 0.9946\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0232 - acc: 0.9926 - val_loss: 0.0182 - val_acc: 0.9948\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0241 - acc: 0.9925 - val_loss: 0.0185 - val_acc: 0.9939\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0234 - acc: 0.9923 - val_loss: 0.0197 - val_acc: 0.9947\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0216 - acc: 0.9930 - val_loss: 0.0190 - val_acc: 0.9944\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0209 - acc: 0.9932 - val_loss: 0.0152 - val_acc: 0.9959\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0204 - acc: 0.9934 - val_loss: 0.0147 - val_acc: 0.9953\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0203 - acc: 0.9931 - val_loss: 0.0168 - val_acc: 0.9942\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0191 - acc: 0.9937 - val_loss: 0.0190 - val_acc: 0.9943\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0186 - acc: 0.9941 - val_loss: 0.0154 - val_acc: 0.9947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc1c4ee6898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrXJoO34mYCU",
        "colab_type": "code",
        "outputId": "8d7e309f-5ee6-4671-dd32-2ea1b4456456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "custom_model = build_model()\n",
        "custom_model.compile(loss=custom_loss(0.02), optimizer=Adam(lr=init_lr), metrics=['accuracy'])\n",
        "\n",
        "print(lamda)\n",
        "custom_model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator),                     \n",
        "                    validation_data=valid_iterator, validation_steps=len(valid_iterator),\n",
        "                    epochs=20, verbose=1, callbacks=[new_lr])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reg. loss 222.95394575595856\n",
            "0.02\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "469/469 [==============================] - 17s 36ms/step - loss: 0.2417 - acc: 0.9255 - val_loss: 0.0671 - val_acc: 0.9775\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0650 - acc: 0.9797 - val_loss: 0.0446 - val_acc: 0.9860\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0508 - acc: 0.9840 - val_loss: 0.0346 - val_acc: 0.9890\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0438 - acc: 0.9860 - val_loss: 0.0390 - val_acc: 0.9870\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0370 - acc: 0.9880 - val_loss: 0.0522 - val_acc: 0.9835\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0355 - acc: 0.9886 - val_loss: 0.0256 - val_acc: 0.9921\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0303 - acc: 0.9901 - val_loss: 0.0258 - val_acc: 0.9913\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0321 - acc: 0.9898 - val_loss: 0.0292 - val_acc: 0.9909\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0267 - acc: 0.9914 - val_loss: 0.0218 - val_acc: 0.9937\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0273 - acc: 0.9910 - val_loss: 0.0218 - val_acc: 0.9933\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0251 - acc: 0.9921 - val_loss: 0.0209 - val_acc: 0.9937\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0240 - acc: 0.9924 - val_loss: 0.0256 - val_acc: 0.9925\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0229 - acc: 0.9925 - val_loss: 0.0269 - val_acc: 0.9930\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0220 - acc: 0.9927 - val_loss: 0.0201 - val_acc: 0.9933\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0225 - acc: 0.9925 - val_loss: 0.0256 - val_acc: 0.9934\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0193 - acc: 0.9937 - val_loss: 0.0178 - val_acc: 0.9937\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0205 - acc: 0.9931 - val_loss: 0.0264 - val_acc: 0.9926\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0202 - acc: 0.9933 - val_loss: 0.0200 - val_acc: 0.9944\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0176 - acc: 0.9941 - val_loss: 0.0223 - val_acc: 0.9936\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0186 - acc: 0.9940 - val_loss: 0.0238 - val_acc: 0.9932\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc1c3cdae10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoYsjsa8odyD",
        "colab_type": "text"
      },
      "source": [
        "## Final run for 40 epochs with lambda 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFCLeb8SLH0c",
        "colab_type": "code",
        "outputId": "31554b66-18ad-4e95-d65a-f8cb358aeba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "custom_model = build_model()\n",
        "custom_model.compile(loss=custom_loss(lamda=0.01), optimizer=Adam(lr=init_lr), metrics=['accuracy'])\n",
        "\n",
        "file = dir + \"Assign5-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "custom_model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator),                     \n",
        "                    validation_data=valid_iterator, validation_steps=len(valid_iterator),\n",
        "                    epochs=40, verbose=1, callbacks=[new_lr, checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reg. loss 223.21972465515137\n",
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "469/469 [==============================] - 19s 40ms/step - loss: 0.2194 - acc: 0.9324 - val_loss: 0.0542 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.98120, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign5/Assign5-01-0.9812.hdf5\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0613 - acc: 0.9805 - val_loss: 0.0356 - val_acc: 0.9885\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.98120 to 0.98850, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign5/Assign5-02-0.9885.hdf5\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0481 - acc: 0.9847 - val_loss: 0.0282 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.98850 to 0.99160, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign5/Assign5-03-0.9916.hdf5\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0413 - acc: 0.9873 - val_loss: 0.0299 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.99160\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0363 - acc: 0.9888 - val_loss: 0.0302 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.99160\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0352 - acc: 0.9891 - val_loss: 0.0297 - val_acc: 0.9912\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.99160\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0315 - acc: 0.9904 - val_loss: 0.0198 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.99160 to 0.99420, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign5/Assign5-07-0.9942.hdf5\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0289 - acc: 0.9908 - val_loss: 0.0192 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.99420 to 0.99480, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign5/Assign5-08-0.9948.hdf5\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0268 - acc: 0.9913 - val_loss: 0.0269 - val_acc: 0.9919\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.99480\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0252 - acc: 0.9919 - val_loss: 0.0182 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.99480\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0228 - acc: 0.9927 - val_loss: 0.0256 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.99480\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0227 - acc: 0.9929 - val_loss: 0.0219 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99480\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0237 - acc: 0.9917 - val_loss: 0.0184 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99480\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0224 - acc: 0.9927 - val_loss: 0.0189 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99480\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0208 - acc: 0.9933 - val_loss: 0.0233 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99480\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0218 - acc: 0.9930 - val_loss: 0.0189 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99480\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0189 - acc: 0.9940 - val_loss: 0.0185 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99480\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0174 - acc: 0.9942 - val_loss: 0.0154 - val_acc: 0.9954\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.99480 to 0.99540, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign5/Assign5-18-0.9954.hdf5\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0205 - acc: 0.9934 - val_loss: 0.0187 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99540\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0177 - acc: 0.9938 - val_loss: 0.0193 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99540\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0186 - acc: 0.9939 - val_loss: 0.0187 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99540\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0184 - acc: 0.9939 - val_loss: 0.0173 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99540\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0161 - acc: 0.9947 - val_loss: 0.0199 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99540\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0157 - acc: 0.9949 - val_loss: 0.0126 - val_acc: 0.9957\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.99540 to 0.99570, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign5/Assign5-24-0.9957.hdf5\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0172 - acc: 0.9942 - val_loss: 0.0187 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99570\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0165 - acc: 0.9945 - val_loss: 0.0165 - val_acc: 0.9958\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.99570 to 0.99580, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign5/Assign5-26-0.9958.hdf5\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0154 - acc: 0.9951 - val_loss: 0.0196 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99580\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0156 - acc: 0.9949 - val_loss: 0.0178 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99580\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0144 - acc: 0.9952 - val_loss: 0.0155 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99580\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0143 - acc: 0.9948 - val_loss: 0.0207 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99580\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0144 - acc: 0.9955 - val_loss: 0.0144 - val_acc: 0.9949\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99580\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0144 - acc: 0.9954 - val_loss: 0.0178 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99580\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0154 - acc: 0.9947 - val_loss: 0.0215 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99580\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0126 - acc: 0.9957 - val_loss: 0.0155 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99580\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0143 - acc: 0.9952 - val_loss: 0.0188 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99580\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0137 - acc: 0.9956 - val_loss: 0.0172 - val_acc: 0.9949\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99580\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0134 - acc: 0.9953 - val_loss: 0.0182 - val_acc: 0.9957\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99580\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0137 - acc: 0.9957 - val_loss: 0.0189 - val_acc: 0.9950\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99580\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0130 - acc: 0.9955 - val_loss: 0.0196 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.99580\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0131 - acc: 0.9958 - val_loss: 0.0182 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.99580\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc1bfd36668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiouEcC5ZY_R",
        "colab_type": "code",
        "outputId": "687a302f-c249-42d3-91ad-ffd12a033bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "custom_model = build_model()\n",
        "custom_model.compile(loss=custom_loss(lamda=0.01), optimizer=Adam(lr=init_lr), metrics=['accuracy'])\n",
        "\n",
        "custom_model.load_weights(dir + \"Assign5-26-0.9958.hdf5\")\n",
        "print(\"Loaded model from disk \")\n",
        "\n",
        "custom_model.evaluate_generator(generator=valid_iterator, steps=len(valid_iterator),verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reg. loss 221.85656595230103\n",
            "Loaded model from disk \n",
            "79/79 [==============================] - 3s 32ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.015816399124125018, 0.9954]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLnVykza3cR8",
        "colab_type": "text"
      },
      "source": [
        "#### Get m misclassified images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2qDl21ozBnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_classes=[]\n",
        "test_classes=[]\n",
        "\n",
        "def get_incorrect_im(X_test, m):\n",
        "  global pred_classes, test_classes\n",
        "    \n",
        "  Y_pred = custom_model.predict(X_test)\n",
        "  pred_classes = np.argmax(Y_pred, axis=1)\n",
        "  test_classes = np.argmax(Y_test, axis=1)\n",
        "\n",
        "  incorrect_im = (test_classes != pred_classes) \n",
        "  \n",
        "  incorrect_idx_list = [i for i, j in enumerate(incorrect_im) if j == True]\n",
        "  idx_list = incorrect_idx_list[:m]\n",
        "  return idx_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQvvg8V37we9",
        "colab_type": "code",
        "outputId": "c3d6aec7-2884-4cc1-c92d-9f015c3f6449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        }
      },
      "source": [
        "incorrect_idx_list = get_incorrect_im(X_test, 25)\n",
        "print(len(incorrect_idx_list), incorrect_idx_list[:10])\n",
        "\n",
        "fig = plt.figure(figsize=(10,12)) # width,height\n",
        "\n",
        "first_incorrect_idx = incorrect_idx_list[0]\n",
        "print(\"For first incorrect image :\")\n",
        "print(\"Predicted\", pred_classes[first_incorrect_idx]) #, Y_pred[first_incorrect_idx])\n",
        "print(\"Actual\", test_classes[first_incorrect_idx]) #, Y_test[first_incorrect_idx])\n",
        "\n",
        "for idx, val in enumerate(incorrect_idx_list):\n",
        "  \n",
        "  # Plot the original test image\n",
        "  sub = fig.add_subplot(5, 5, idx+1) # nrows, ncols, index\n",
        "  \n",
        "  label = str(pred_classes[val]) + \":\" + str(test_classes[val])\n",
        "  sub.set_title(label)\n",
        "  sub.imshow(x_test[val], cmap=plt.get_cmap('gray'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "For first incorrect image :\n",
            "Predicted 8\n",
            "Actual 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAKrCAYAAADVgAicAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x864 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}