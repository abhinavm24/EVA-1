{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assign6B.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkwXnw9OfHZl",
        "colab_type": "code",
        "outputId": "4fe3c3c8-616b-4e24-dc64-50cc4ebf6f75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers import Input, Activation, Flatten, Dense, Dropout, BatchNormalization, SeparableConv2D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from google.colab import drive\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "% matplotlib inline\n",
        "np.random.seed(2017)\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "dir=\"/content/gdrive/My Drive/Colab Notebooks/EVA/Weights/\"\n",
        "!ls\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHpnoCHZfO8g",
        "colab_type": "code",
        "outputId": "62fbe1fd-cd36-4076-bc22-fb2a083b6b0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "(train_features, train_labels), (test_features, test_labels) = cifar10.load_data()\n",
        "print(train_features.shape)\n",
        "print(test_features.shape)\n",
        "\n",
        "num_train, img_rows, img_cols, img_channels =  train_features.shape\n",
        "num_test, _, _, _ =  test_features.shape\n",
        "\n",
        "# num_classes=10\n",
        "num_classes = len(np.unique(train_labels))\n",
        "print(num_train, num_test, img_channels)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "50000 10000 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5c5nDvxm6zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = train_features.astype('float32')/255\n",
        "test_features = test_features.astype('float32')/255\n",
        "\n",
        "# convert class labels to binary class labels\n",
        "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = np_utils.to_categorical(test_labels, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxyOeEhnrcSJ",
        "colab_type": "text"
      },
      "source": [
        "### Model from 6A : For 20 epochs, best validation accuracy is 81.43% at epoch 18, when training acc is 95.43%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9mqNpVPSWreA",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Conv2D(48, 3, padding='same', input_shape=(32, 32, 3))) #rf 3\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Conv2D(96, 3, padding='same')) \n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))\n",
        "  \n",
        "  model.add(Conv2D(64, 1 ))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2))) #16, rf10  \n",
        "\n",
        "  model.add(Conv2D(96, 3, padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Conv2D(192, 3, padding='same')) #16, rf14\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))\n",
        "  \n",
        "  model.add(Conv2D(128, 1))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2))) #8, rf28\n",
        "  \n",
        "  model.add(Conv2D(192, 3, padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Conv2D(384, 3, padding='same'))#8, rf32\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))\n",
        "  \n",
        "  model.add(Conv2D(200, 1))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2))) #4, rf64\n",
        "   \n",
        "  model.add(Conv2D(100, 1))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(10, 4))\n",
        "    \n",
        "  model.add(Flatten())\n",
        "  model.add(Activation('softmax'))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUl8BcC2W0Y2",
        "colab_type": "code",
        "outputId": "60fb92dc-409b-473c-9b62-f350f480690a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = build_model()\n",
        "model.summary()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_40 (Conv2D)           (None, 32, 32, 48)        1344      \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 32, 32, 48)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 32, 32, 48)        192       \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 32, 32, 48)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 32, 32, 96)        41568     \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 32, 32, 96)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 32, 32, 96)        384       \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 32, 32, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_42 (Conv2D)           (None, 32, 32, 64)        6208      \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 16, 16, 96)        55392     \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 16, 16, 96)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 16, 16, 96)        384       \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 16, 16, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 16, 16, 192)       166080    \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 16, 16, 192)       768       \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 16, 16, 128)       24704     \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 8, 8, 192)         221376    \n",
            "_________________________________________________________________\n",
            "activation_48 (Activation)   (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 8, 8, 192)         768       \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 8, 8, 384)         663936    \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 8, 8, 384)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 8, 8, 384)         1536      \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 8, 8, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 8, 8, 200)         77000     \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 8, 8, 200)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 4, 4, 200)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 4, 4, 100)         20100     \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 4, 4, 100)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 1, 1, 10)          16010     \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,297,750\n",
            "Trainable params: 1,295,734\n",
            "Non-trainable params: 2,016\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLYcQoakXDks",
        "colab_type": "code",
        "outputId": "4344bf6d-2b96-4702-a75f-3c5d8ee726d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "model.fit(train_features, train_labels, batch_size=128, epochs=20, verbose=1,\n",
        "          validation_data=(test_features, test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 29s 571us/step - loss: 1.3099 - acc: 0.5301 - val_loss: 1.0711 - val_acc: 0.6378\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 26s 525us/step - loss: 0.8384 - acc: 0.7037 - val_loss: 1.1115 - val_acc: 0.6393\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 27s 530us/step - loss: 0.6806 - acc: 0.7592 - val_loss: 0.8594 - val_acc: 0.7173\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 27s 533us/step - loss: 0.5799 - acc: 0.7977 - val_loss: 0.7789 - val_acc: 0.7338\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 26s 526us/step - loss: 0.5112 - acc: 0.8225 - val_loss: 0.7624 - val_acc: 0.7459\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 26s 529us/step - loss: 0.4531 - acc: 0.8408 - val_loss: 0.9219 - val_acc: 0.7092\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 26s 528us/step - loss: 0.3991 - acc: 0.8595 - val_loss: 0.6941 - val_acc: 0.7660\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 26s 526us/step - loss: 0.3528 - acc: 0.8746 - val_loss: 0.6324 - val_acc: 0.7910\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 26s 526us/step - loss: 0.3086 - acc: 0.8901 - val_loss: 0.6283 - val_acc: 0.8088\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 26s 527us/step - loss: 0.2741 - acc: 0.9020 - val_loss: 0.6874 - val_acc: 0.7894\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 26s 529us/step - loss: 0.2412 - acc: 0.9135 - val_loss: 0.6676 - val_acc: 0.7958\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 26s 526us/step - loss: 0.2132 - acc: 0.9232 - val_loss: 0.6923 - val_acc: 0.7977\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 26s 526us/step - loss: 0.1991 - acc: 0.9277 - val_loss: 0.8163 - val_acc: 0.7851\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 26s 527us/step - loss: 0.1791 - acc: 0.9371 - val_loss: 0.7613 - val_acc: 0.7957\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 26s 525us/step - loss: 0.1606 - acc: 0.9417 - val_loss: 0.7653 - val_acc: 0.8073\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 26s 525us/step - loss: 0.1570 - acc: 0.9441 - val_loss: 0.7524 - val_acc: 0.8097\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 26s 525us/step - loss: 0.1369 - acc: 0.9498 - val_loss: 0.7883 - val_acc: 0.8033\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 26s 526us/step - loss: 0.1267 - acc: 0.9543 - val_loss: 0.7883 - val_acc: 0.8143\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 26s 527us/step - loss: 0.1258 - acc: 0.9545 - val_loss: 0.7733 - val_acc: 0.8098\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 26s 525us/step - loss: 0.1204 - acc: 0.9568 - val_loss: 0.8178 - val_acc: 0.8074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa7a41d6da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqjx9EpdjCtA",
        "colab_type": "text"
      },
      "source": [
        "#### Added depthwise separable conv in 3 channel-heavy conv layers. Parameters reduced from 1.3m to 0.52m. For 20 epochs, best validation accuracy is 81.89% at epoch 17, when training acc is 91.27%. \n",
        "With less than half parameters, no drop in accuracy. Since training accuracy has dropped from 95 to 91%, overfitting is reduced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvbF_h8uFjc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_first_model():\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Conv2D(48, 3, padding='same', input_shape=(32, 32, 3))) #rf 3\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(SeparableConv2D(96, 3, padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))\n",
        "  \n",
        "  model.add(Conv2D(64, 1 ))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2))) #16, rf10  \n",
        "\n",
        "  model.add(Conv2D(96, 3, padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(SeparableConv2D(192, 3, padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))\n",
        "  \n",
        "  model.add(Conv2D(128, 1))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2))) #8, rf28\n",
        "  \n",
        "  model.add(Conv2D(192, 3, padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(SeparableConv2D(384, 3, padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))\n",
        "  \n",
        "  model.add(Conv2D(200, 1))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2))) #4, rf64\n",
        "   \n",
        "  model.add(Conv2D(100, 1))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(10, 4))\n",
        "    \n",
        "  model.add(Flatten())\n",
        "  model.add(Activation('softmax'))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF0lVxiPHbH5",
        "colab_type": "code",
        "outputId": "e661d9ff-9eba-46b2-ada2-77dd6fc8aed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model=build_first_model()\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_59 (Conv2D)           (None, 32, 32, 48)        1344      \n",
            "_________________________________________________________________\n",
            "activation_64 (Activation)   (None, 32, 32, 48)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 32, 32, 48)        192       \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 32, 32, 48)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_4 (Separabl (None, 32, 32, 96)        5136      \n",
            "_________________________________________________________________\n",
            "activation_65 (Activation)   (None, 32, 32, 96)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 32, 32, 96)        384       \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 32, 32, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_60 (Conv2D)           (None, 32, 32, 64)        6208      \n",
            "_________________________________________________________________\n",
            "activation_66 (Activation)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_61 (Conv2D)           (None, 16, 16, 96)        55392     \n",
            "_________________________________________________________________\n",
            "activation_67 (Activation)   (None, 16, 16, 96)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 16, 16, 96)        384       \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 16, 16, 96)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_5 (Separabl (None, 16, 16, 192)       19488     \n",
            "_________________________________________________________________\n",
            "activation_68 (Activation)   (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 16, 16, 192)       768       \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_62 (Conv2D)           (None, 16, 16, 128)       24704     \n",
            "_________________________________________________________________\n",
            "activation_69 (Activation)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_63 (Conv2D)           (None, 8, 8, 192)         221376    \n",
            "_________________________________________________________________\n",
            "activation_70 (Activation)   (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 8, 8, 192)         768       \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_6 (Separabl (None, 8, 8, 384)         75840     \n",
            "_________________________________________________________________\n",
            "activation_71 (Activation)   (None, 8, 8, 384)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 8, 8, 384)         1536      \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 8, 8, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_64 (Conv2D)           (None, 8, 8, 200)         77000     \n",
            "_________________________________________________________________\n",
            "activation_72 (Activation)   (None, 8, 8, 200)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 4, 4, 200)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_65 (Conv2D)           (None, 4, 4, 100)         20100     \n",
            "_________________________________________________________________\n",
            "activation_73 (Activation)   (None, 4, 4, 100)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_66 (Conv2D)           (None, 1, 1, 10)          16010     \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_74 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 526,630\n",
            "Trainable params: 524,614\n",
            "Non-trainable params: 2,016\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGIk1N19abWk",
        "colab_type": "code",
        "outputId": "87d7b90f-0645-4872-c6dc-6fb873a03729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "model.fit(train_features, train_labels, batch_size=128, epochs=20, verbose=1,\n",
        "          validation_data=(test_features, test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 26s 514us/step - loss: 1.3540 - acc: 0.5139 - val_loss: 1.0930 - val_acc: 0.6179\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 23s 454us/step - loss: 0.9265 - acc: 0.6720 - val_loss: 0.9201 - val_acc: 0.6807\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 23s 457us/step - loss: 0.7658 - acc: 0.7293 - val_loss: 0.8419 - val_acc: 0.7053\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 23s 454us/step - loss: 0.6697 - acc: 0.7666 - val_loss: 0.9164 - val_acc: 0.6995\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 23s 453us/step - loss: 0.5920 - acc: 0.7930 - val_loss: 0.8190 - val_acc: 0.7187\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 23s 453us/step - loss: 0.5409 - acc: 0.8098 - val_loss: 0.8564 - val_acc: 0.7173\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 23s 454us/step - loss: 0.4933 - acc: 0.8265 - val_loss: 0.7601 - val_acc: 0.7591\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 23s 453us/step - loss: 0.4565 - acc: 0.8389 - val_loss: 0.6704 - val_acc: 0.7814\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 23s 455us/step - loss: 0.4231 - acc: 0.8509 - val_loss: 0.6770 - val_acc: 0.7740\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 23s 452us/step - loss: 0.3923 - acc: 0.8615 - val_loss: 0.6429 - val_acc: 0.7922\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 23s 453us/step - loss: 0.3616 - acc: 0.8710 - val_loss: 0.6162 - val_acc: 0.7949\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 23s 456us/step - loss: 0.3369 - acc: 0.8811 - val_loss: 0.6585 - val_acc: 0.7945\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 23s 452us/step - loss: 0.3153 - acc: 0.8878 - val_loss: 0.6441 - val_acc: 0.7987\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 23s 453us/step - loss: 0.2885 - acc: 0.8989 - val_loss: 0.6473 - val_acc: 0.7976\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 23s 454us/step - loss: 0.2743 - acc: 0.9040 - val_loss: 0.6410 - val_acc: 0.8074\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 23s 453us/step - loss: 0.2601 - acc: 0.9068 - val_loss: 0.6262 - val_acc: 0.8179\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 23s 454us/step - loss: 0.2464 - acc: 0.9127 - val_loss: 0.5950 - val_acc: 0.8189\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 23s 453us/step - loss: 0.2281 - acc: 0.9167 - val_loss: 0.6835 - val_acc: 0.8116\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 23s 454us/step - loss: 0.2159 - acc: 0.9218 - val_loss: 0.7011 - val_acc: 0.8109\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 23s 455us/step - loss: 0.2164 - acc: 0.9218 - val_loss: 0.6528 - val_acc: 0.8160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa6b15bbf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-5Yaq0_5fnC",
        "colab_type": "text"
      },
      "source": [
        "#### Added spatial separable conv in 3 channel-heavy conv. layers. Parameters reduced from 1.3m to 0.862m.\n",
        "For 20 epochs, best validation accuracy is 81.31% at epoch 16, when training acc is 93.88%. Only slight drop in accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYWU3TXWQKHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spatial_sep_conv(model, filter_x, filter_out):\n",
        "    l1 = Conv2D(filter_x, (3, 1), activation='relu', padding='same')\n",
        "    l2 = Conv2D(filter_out, (1,3), activation='relu', padding='same')\n",
        "    model.add(l1)\n",
        "    model.add(l2)\n",
        "    print(type(l1), l1.output_shape, l2.output_shape)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TjU0WF8Sanh1",
        "colab": {}
      },
      "source": [
        "def build_second_model():\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Conv2D(48, 3, padding='same', input_shape=(32, 32, 3))) #rf 3\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))  \n",
        "  model = spatial_sep_conv(model, 48, 96)\n",
        "  model.add(BatchNormalization())\n",
        "    \n",
        "  model.add(Conv2D(64, 1 ))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2))) #16, rf10  \n",
        "\n",
        "  model.add(Conv2D(96, 3, padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))\n",
        "  model = spatial_sep_conv(model, 96, 192)\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "    \n",
        "  model.add(Conv2D(128, 1))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2))) #8, rf28\n",
        "  \n",
        "  model.add(Conv2D(192, 3, padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.1))\n",
        "  model = spatial_sep_conv(model, 192, 384)\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "    \n",
        "  model.add(Conv2D(200, 1))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2))) #4, rf64\n",
        "   \n",
        "  model.add(Conv2D(100, 1))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(10, 4))\n",
        "    \n",
        "  model.add(Flatten())\n",
        "  model.add(Activation('softmax'))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOUe2-vYHCfT",
        "colab_type": "code",
        "outputId": "87327c5a-a1fe-46ab-eeff-45fd42612b6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model=build_second_model()\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 32, 32, 48) (None, 32, 32, 96)\n",
            "(None, 16, 16, 96) (None, 16, 16, 192)\n",
            "(None, 8, 8, 192) (None, 8, 8, 384)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_32 (Conv2D)           (None, 32, 32, 48)        1344      \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 32, 32, 48)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 32, 32, 48)        192       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 32, 32, 48)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 32, 32, 48)        6960      \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 32, 32, 96)        13920     \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 32, 32, 96)        384       \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 32, 32, 64)        6208      \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 16, 16, 96)        55392     \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 16, 16, 96)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 16, 16, 96)        384       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 16, 16, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 16, 16, 96)        27744     \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 16, 16, 192)       55488     \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 16, 16, 192)       768       \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 16, 16, 128)       24704     \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 8, 8, 192)         221376    \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 8, 8, 192)         768       \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 8, 8, 192)         110784    \n",
            "_________________________________________________________________\n",
            "conv2d_42 (Conv2D)           (None, 8, 8, 384)         221568    \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 8, 8, 384)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 8, 8, 384)         1536      \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 8, 8, 200)         77000     \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 8, 8, 200)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 200)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 4, 4, 100)         20100     \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 4, 4, 100)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 1, 1, 10)          16010     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 862,630\n",
            "Trainable params: 860,614\n",
            "Non-trainable params: 2,016\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNOO9G-huuQE",
        "colab_type": "code",
        "outputId": "b448a03e-5101-4782-f7ff-e91882480177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "model.fit(train_features, train_labels, batch_size=128, epochs=20, verbose=1,\n",
        "          validation_data=(test_features, test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 58s 1ms/step - loss: 1.4077 - acc: 0.4925 - val_loss: 1.3704 - val_acc: 0.5397\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 56s 1ms/step - loss: 0.9788 - acc: 0.6530 - val_loss: 1.1442 - val_acc: 0.6281\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 57s 1ms/step - loss: 0.7736 - acc: 0.7280 - val_loss: 1.0112 - val_acc: 0.6622\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 57s 1ms/step - loss: 0.6522 - acc: 0.7723 - val_loss: 1.0051 - val_acc: 0.6797\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 57s 1ms/step - loss: 0.5716 - acc: 0.7982 - val_loss: 0.6610 - val_acc: 0.7740\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 57s 1ms/step - loss: 0.5056 - acc: 0.8229 - val_loss: 0.7793 - val_acc: 0.7363\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 57s 1ms/step - loss: 0.4514 - acc: 0.8413 - val_loss: 0.8771 - val_acc: 0.7125\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 57s 1ms/step - loss: 0.4056 - acc: 0.8575 - val_loss: 0.6567 - val_acc: 0.7886\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 56s 1ms/step - loss: 0.3565 - acc: 0.8747 - val_loss: 0.6772 - val_acc: 0.7861\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 56s 1ms/step - loss: 0.3286 - acc: 0.8825 - val_loss: 0.8502 - val_acc: 0.7505\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 56s 1ms/step - loss: 0.2961 - acc: 0.8957 - val_loss: 0.6889 - val_acc: 0.7773\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 56s 1ms/step - loss: 0.2639 - acc: 0.9054 - val_loss: 0.8440 - val_acc: 0.7475\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 56s 1ms/step - loss: 0.2346 - acc: 0.9168 - val_loss: 0.7007 - val_acc: 0.7966\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 57s 1ms/step - loss: 0.2141 - acc: 0.9238 - val_loss: 0.6826 - val_acc: 0.7963\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 57s 1ms/step - loss: 0.1905 - acc: 0.9318 - val_loss: 0.6791 - val_acc: 0.8074\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 56s 1ms/step - loss: 0.1721 - acc: 0.9388 - val_loss: 0.6780 - val_acc: 0.8131\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 57s 1ms/step - loss: 0.1641 - acc: 0.9410 - val_loss: 0.8583 - val_acc: 0.7953\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 57s 1ms/step - loss: 0.1467 - acc: 0.9470 - val_loss: 0.8363 - val_acc: 0.8105\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 57s 1ms/step - loss: 0.1402 - acc: 0.9489 - val_loss: 0.8492 - val_acc: 0.7937\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 56s 1ms/step - loss: 0.1379 - acc: 0.9517 - val_loss: 0.8132 - val_acc: 0.8011\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0697fdf630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5E5D2MfxebL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spatial_sep_conv2(filter_x, filter_out, layer_in):\n",
        "    l1 = Conv2D(filter_x, (3,1), activation='relu', padding='same')(layer_in)\n",
        "    l2 = Conv2D(filter_out, (1,3), activation='relu', padding='same')(l1)\n",
        "    print(type(l1), l1.shape, l2.shape)\n",
        "    return l2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEg9bptwb1MH",
        "colab_type": "text"
      },
      "source": [
        "#### Using 5 types of conv : Normal (3x3), spatially separable, depthwise separable, grouped conv. Parameters are inc to 2.8m from 1.3m.\n",
        "### Best validation accuracy in 50 epochs is 81.7% at epoch 48, with training accuracy at 98.13%. This model shows distinct overfitting with training accuracy > 98%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e462a273-7494-4f47-aed5-af1fdff6974a",
        "id": "smDoQUDvZO1r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "input = Input(shape=(img_rows, img_cols, img_channels,))\n",
        "\n",
        "layer1 = Conv2D(32, 3, padding='same', activation='relu')(input)\n",
        "layer1 = BatchNormalization()(layer1)\n",
        "layer1 = Dropout(0.1)(layer1) \n",
        "print(\"Layer1 shape\", layer1.shape)\n",
        "layer2 = Conv2D(64, 3, padding='same', activation='relu')(layer1)\n",
        "layer2 = BatchNormalization()(layer2)\n",
        "layer2 = Dropout(0.1)(layer2)\n",
        "print(\"Layer2 shape\", layer2.shape)\n",
        "\n",
        "layer3 = Conv2D(48, 1, padding='same', activation='relu')(layer2)\n",
        "layer3 = MaxPooling2D(pool_size=(2, 2))(layer3)\n",
        "\n",
        "# Spatially Separable Convolution\n",
        "layer4 = spatial_sep_conv2(48, 64, layer3)\n",
        "layer4 = BatchNormalization()(layer4)\n",
        "layer4 = Dropout(0.1)(layer4)\n",
        "print(\"Layer4 spatial sep conv shape\", layer4.shape)\n",
        "\n",
        "# Depthwise Separable Convolution\n",
        "layer5 = SeparableConv2D(128, 3, padding='same', activation='relu')(layer4)\n",
        "layer5 = BatchNormalization()(layer5)\n",
        "layer5 = Dropout(0.1)(layer5)\n",
        "print(\"Layer5 depth sep conv shape\", layer5.shape)\n",
        "\n",
        "layer6 = Conv2D(96, 1, padding='same', activation='relu')(layer5)\n",
        "layer6 = MaxPooling2D(pool_size=(2, 2))(layer6)\n",
        "temp1 = layer6\n",
        "\n",
        "# Group conv 1\n",
        "layer7 = Conv2D(128, 3, padding='same', activation='relu')(layer6)\n",
        "layer7 = BatchNormalization()(layer7)\n",
        "layer7b = Conv2D(128, 5, padding='same', activation='relu')(temp1)\n",
        "layer7b = BatchNormalization()(layer7b)\n",
        "print(\"Layer7 shape\", layer7.shape, layer7b.shape)\n",
        "layer8 = concatenate([layer7, layer7b])\n",
        "print(\"After concat, layer8 shape\", layer8.shape)\n",
        "layer8 = Dropout(0.1)(layer8)\n",
        "temp2 = layer8\n",
        "\n",
        "# Group conv 2\n",
        "layer9 = Conv2D(256, 3, padding='same', activation='relu')(layer8)\n",
        "layer9 = BatchNormalization()(layer9)\n",
        "layer9b = Conv2D(256, 5, padding='same', dilation_rate=(2,2), activation='relu')(temp2)\n",
        "layer9b = BatchNormalization()(layer9b)\n",
        "print(\"Layer9 shape\", layer9.shape, layer9b.shape)\n",
        "layer10 = concatenate([layer9, layer9b])\n",
        "print(\"After concat, layer10 shape\", layer10.shape)\n",
        "layer10 = Dropout(0.1)(layer10)\n",
        "\n",
        "layer11 = Conv2D(250, 1, activation='relu')(layer10)\n",
        "layer11 = MaxPooling2D(pool_size=(2, 2))(layer11)\n",
        "print(\"Layer11 shape after MP\", layer11.shape)\n",
        "\n",
        "layer12 = Conv2D(100, 1, activation='relu')(layer11)\n",
        "layer13 = Conv2D(num_classes, 4)(layer12)\n",
        "print(\"Layer12,13 shape\", layer12.shape, layer13.shape)\n",
        "\n",
        "output = Flatten()(layer13)\n",
        "output = Activation('softmax')(output)\n",
        "#, activation='softmax'\n",
        "print(\"Output shape\", output.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Layer1 shape (?, 32, 32, 32)\n",
            "Layer2 shape (?, 32, 32, 64)\n",
            "<class 'tensorflow.python.framework.ops.Tensor'> (?, 16, 16, 48) (?, 16, 16, 64)\n",
            "Layer4 spatial sep conv shape (?, 16, 16, 64)\n",
            "Layer5 depth sep conv shape (?, 16, 16, 128)\n",
            "Layer7 shape (?, 8, 8, 128) (?, 8, 8, 128)\n",
            "After concat, layer8 shape (?, 8, 8, 256)\n",
            "Layer9 shape (?, 8, 8, 256) (?, 8, 8, 256)\n",
            "After concat, layer10 shape (?, 8, 8, 512)\n",
            "Layer11 shape after MP (?, 4, 4, 250)\n",
            "Layer12,13 shape (?, 4, 4, 100) (?, 1, 1, 10)\n",
            "Output shape (?, ?)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhyv11Dmg7iB",
        "colab_type": "code",
        "outputId": "be7f2040-ef43-4bd7-aa95-2b23821b4e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 32)   896         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 32)   128         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 32)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 32, 32, 64)   18496       dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 64)   256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32, 32, 64)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 48)   3120        dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 48)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 48)   6960        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 64)   9280        conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_2 (SeparableCo (None, 16, 16, 128)  8896        dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         separable_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 16, 16, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 96)   12384       dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 8, 8, 96)     0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 128)    110720      max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 128)    307328      max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 128)    512         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 128)    512         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 8, 8, 256)    0           batch_normalization_13[0][0]     \n",
            "                                                                 batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 8, 8, 256)    0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 256)    590080      dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 256)    1638656     dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 256)    1024        conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 256)    1024        conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 8, 8, 512)    0           batch_normalization_15[0][0]     \n",
            "                                                                 batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 8, 8, 512)    0           concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 250)    128250      dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 4, 4, 250)    0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 4, 4, 100)    25100       max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 1, 1, 10)     16010       conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 10)           0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 10)           0           flatten_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,880,400\n",
            "Trainable params: 2,878,288\n",
            "Non-trainable params: 2,112\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGom6CoMoVx3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4109cc76-96d1-4a3b-f0a6-c218b1777fcf"
      },
      "source": [
        "file = dir + \"Assign6B.{epoch:02d}-{val_acc:.3f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "model_info = model.fit(train_features, train_labels, batch_size=batch_size, epochs=50, verbose=1,\n",
        "          validation_data=(test_features, test_labels), callbacks=[checkpoint])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 86s 2ms/step - loss: 1.3665 - acc: 0.5144 - val_loss: 1.3767 - val_acc: 0.5361\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.53610, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign6B.01-0.536.hdf5\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.8617 - acc: 0.6944 - val_loss: 0.9191 - val_acc: 0.6776\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.53610 to 0.67760, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign6B.02-0.678.hdf5\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.6825 - acc: 0.7599 - val_loss: 0.7744 - val_acc: 0.7318\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.67760 to 0.73180, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign6B.03-0.732.hdf5\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.5780 - acc: 0.7970 - val_loss: 0.8078 - val_acc: 0.7207\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.73180\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.4912 - acc: 0.8280 - val_loss: 0.9058 - val_acc: 0.7241\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.73180\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.4168 - acc: 0.8528 - val_loss: 0.7136 - val_acc: 0.7763\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.73180 to 0.77630, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign6B.06-0.776.hdf5\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.3520 - acc: 0.8747 - val_loss: 0.7328 - val_acc: 0.7706\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.77630\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.2995 - acc: 0.8934 - val_loss: 0.8720 - val_acc: 0.7495\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.77630\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.2445 - acc: 0.9133 - val_loss: 0.7377 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.77630 to 0.78120, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign6B.09-0.781.hdf5\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.2093 - acc: 0.9248 - val_loss: 0.7712 - val_acc: 0.7824\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.78120 to 0.78240, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign6B.10-0.782.hdf5\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.1826 - acc: 0.9356 - val_loss: 0.8531 - val_acc: 0.7744\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.78240\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.1545 - acc: 0.9449 - val_loss: 0.8368 - val_acc: 0.7728\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.78240\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.1435 - acc: 0.9489 - val_loss: 0.8412 - val_acc: 0.7814\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.78240\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.1327 - acc: 0.9534 - val_loss: 0.9138 - val_acc: 0.7885\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.78240 to 0.78850, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign6B.14-0.788.hdf5\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.1263 - acc: 0.9552 - val_loss: 0.8887 - val_acc: 0.8002\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.78850 to 0.80020, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign6B.15-0.800.hdf5\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.1139 - acc: 0.9598 - val_loss: 0.8903 - val_acc: 0.7963\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.80020\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.1016 - acc: 0.9649 - val_loss: 0.9283 - val_acc: 0.7872\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.80020\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.1090 - acc: 0.9617 - val_loss: 0.9058 - val_acc: 0.7992\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.80020\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0973 - acc: 0.9658 - val_loss: 0.9792 - val_acc: 0.7881\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.80020\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0938 - acc: 0.9666 - val_loss: 0.9706 - val_acc: 0.7937\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.80020\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0980 - acc: 0.9662 - val_loss: 0.9895 - val_acc: 0.7924\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.80020\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0911 - acc: 0.9684 - val_loss: 0.8859 - val_acc: 0.8111\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.80020 to 0.81110, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign6B.22-0.811.hdf5\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0748 - acc: 0.9740 - val_loss: 1.0595 - val_acc: 0.7910\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.81110\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0799 - acc: 0.9723 - val_loss: 0.9743 - val_acc: 0.8019\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.81110\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0780 - acc: 0.9725 - val_loss: 1.0839 - val_acc: 0.7925\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.81110\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0777 - acc: 0.9722 - val_loss: 1.0705 - val_acc: 0.7961\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.81110\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0833 - acc: 0.9705 - val_loss: 1.0240 - val_acc: 0.7946\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.81110\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0669 - acc: 0.9765 - val_loss: 1.0820 - val_acc: 0.7931\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.81110\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0783 - acc: 0.9729 - val_loss: 1.0346 - val_acc: 0.8001\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.81110\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0707 - acc: 0.9757 - val_loss: 1.1691 - val_acc: 0.7889\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.81110\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0726 - acc: 0.9756 - val_loss: 1.0187 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.81110\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0627 - acc: 0.9785 - val_loss: 1.1066 - val_acc: 0.8017\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.81110\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0619 - acc: 0.9792 - val_loss: 1.0727 - val_acc: 0.8052\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.81110\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0634 - acc: 0.9785 - val_loss: 1.1766 - val_acc: 0.7955\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.81110\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0621 - acc: 0.9790 - val_loss: 1.0181 - val_acc: 0.8079\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.81110\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0605 - acc: 0.9793 - val_loss: 1.1358 - val_acc: 0.7999\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.81110\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0629 - acc: 0.9789 - val_loss: 1.1201 - val_acc: 0.8054\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.81110\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0621 - acc: 0.9792 - val_loss: 1.0178 - val_acc: 0.8130\n",
            "\n",
            "Epoch 00038: val_acc improved from 0.81110 to 0.81300, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign6B.38-0.813.hdf5\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0594 - acc: 0.9802 - val_loss: 1.0652 - val_acc: 0.8085\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.81300\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0598 - acc: 0.9799 - val_loss: 1.1702 - val_acc: 0.8081\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.81300\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0565 - acc: 0.9805 - val_loss: 1.0831 - val_acc: 0.8063\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.81300\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0548 - acc: 0.9812 - val_loss: 1.0939 - val_acc: 0.8045\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.81300\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0546 - acc: 0.9816 - val_loss: 1.2795 - val_acc: 0.7959\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.81300\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0552 - acc: 0.9815 - val_loss: 1.1086 - val_acc: 0.8130\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.81300\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0527 - acc: 0.9829 - val_loss: 1.1098 - val_acc: 0.8139\n",
            "\n",
            "Epoch 00045: val_acc improved from 0.81300 to 0.81390, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign6B.45-0.814.hdf5\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0517 - acc: 0.9830 - val_loss: 1.1767 - val_acc: 0.7946\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.81390\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0527 - acc: 0.9830 - val_loss: 1.1954 - val_acc: 0.8016\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.81390\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0542 - acc: 0.9813 - val_loss: 1.1052 - val_acc: 0.8166\n",
            "\n",
            "Epoch 00048: val_acc improved from 0.81390 to 0.81660, saving model to /content/gdrive/My Drive/Colab Notebooks/EVA/Weights/Assign6B.48-0.817.hdf5\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0449 - acc: 0.9861 - val_loss: 1.1344 - val_acc: 0.8152\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.81660\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 81s 2ms/step - loss: 0.0511 - acc: 0.9827 - val_loss: 1.1405 - val_acc: 0.8090\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.81660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmfsk76-fadV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "aa64246d-811e-4c82-fe61-6415bdcad23b"
      },
      "source": [
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(10,4))\n",
        "    \n",
        "    # Plot training & validation accuracy values\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    #axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    \n",
        "    # Plot training & validation loss values\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    #axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()\n",
        "    \n",
        "plot_model_history(model_info)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAEWCAYAAADIE4vrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMcY47H8ABDBcX7eEH8oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02bkyb4pdJPu",
        "colab_type": "code",
        "outputId": "8f354543-3361-4971-cb4e-bf4c1cba40f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "model.load_weights(dir + \"Assign6B.48-0.817.hdf5\")\n",
        "print(\"Loaded model from disk \")\n",
        "\n",
        "score = model.evaluate(test_features, test_labels)\n",
        "print(score)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk \n",
            "10000/10000 [==============================] - 6s 552us/step\n",
            "[1.1051891950309276, 0.8166]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}